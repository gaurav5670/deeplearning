# Hints

* Repeated weight adjustment => hidden units begin to represent the features in the input data and the regularities in the *task domain*
* What at circumstances (input) should the hidden units be active decides what they represent
* Chain rule for back-propagation
* Breaking symmetry with bias
* Local minima vs Global minima
* Acceleration method in gradient descent for faster convergence
* With increase in hidden units => increased weights => more local minima to choose from 