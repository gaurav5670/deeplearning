# Back to the Roots

The curse of dimensionality has been a major hurdle in science and engineering. With linear increase in dimensionality of a natural phenomenon, learning complexity grows exponentially. Feature extraction or dimensional reduction was used to project the data in a lower dimensional space, for which tools are available for analysis and classification. But the feature extraction process is human-engineered, requires extensive knowledge of the domain and most importantly, it is domain-specific. 

## Motivation

Neuroscience study on mammalian neocortex provided new insight to information processing in the brain. Scientists observed sensory signals being passed through a hierarchy of modules, instead of preprocessing. This hierarchy of modules, over time, learn to represent the regularities in the signal. This motivated researchers, to start a separate field of machine learning, called deep learning, which tries to process information similar to that of neorcortex.[[1](http://web.eecs.utk.edu/~itamar/Papers/CIM2010.pdf)]

> Capturing spatiotemporal dependencies, based on regularities in the observations, is therefore viewed as a fundamental goal for deep learning systems.

### Types 

1. Unsupervised or Generative Networks
2. Supervised or Discriminative Networks
3. Hybrid Deep Networks

## Notes

1. Unsupervised Learning = Representation Learning

## References

1. [A New Frontier in AI research](http://web.eecs.utk.edu/~itamar/Papers/CIM2010.pdf)

