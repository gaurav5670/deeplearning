# Standalone Deep Learning Guide

This document is my unorthodox attempt to create dynamic knowledge on deep learning for beginners and people overwhelmed by TMI on deep learning. It is unorthodox because I will start with Recurrent Neural Network and fill up the blanks around it as read and experiment more. I must confess. The primary goal of this project is not to help people stuggling with deep learning concepts. It is a way for me to actually understand and quantify what I know in this domain. I am not sure if I'll be successful in completing this, but I have convinced myself that this is the right thing to do now, to get out this rut I got myself into. 

Let me list out what I actually know.

* Basics of Neural Networks
* Convolutional Neural Networks concepts (*from CS231*)
* Learning, Hyperparameters and Generalization (*from Andrew Ng's ML course, ML chapter on Bengio's DL book*)
* Caffe Basics and Usage (*applied lenet5 for tamil OCR*)
* Vector Space Models and Advanced Word Representation theory (*from CS224d*)
* Linear Algebra concepts for DL, PCA (*Bengio's book*)
* Probability and Statistics (*Bengio's book*)

Now, what I don't know.

**Models**

* Recurrent Neural Network
* LSTM : Long Short Term Memory
* Autoencoders
* Deep Restricted Boltzmann machines
* Deep Belief Network
* Bi-directional RNN

**Frameworks**

* Theano 
* Torch 
* TensorFlow

**Concepts**

* Vanishing Gradient Problem
* RProp : Resilient Backpropagation 
* Neural Abstraction Pyramid
* [Ancestral Pass of DBN](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)
* [Liquid State Machines](https://en.wikipedia.org/wiki/Liquid_state_machines)

*Nov 16 2015 : Continuing RNN*

* Backpropagation Through Time (BPTT)
* Recursive Neural Network (Generalization of RNN)
* [Turing-complete](http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf)

* Softmax classifier
* Cross Entropy error
* [RMSProp](http://arxiv.org/abs/1502.04390)
* [Gradient Checking](http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization)

*Nov 17 2015 : Still Continuing RNN*

* [Soft and fully differentiable Attention Model](http://arxiv.org/abs/1410.5401)
* 

## Reading List

1. [http://ufldl.stanford.edu/tutorial/](http://ufldl.stanford.edu/tutorial/)
2. [Learning From Data](http://work.caltech.edu/previous.html)
3. [The Neural](https://theneural.wordpress.com/)